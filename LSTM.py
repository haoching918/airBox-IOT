# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zRFg_HiMRtfQ08z7_ed5FyzWc25z-h5u
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, ConcatDataset
import pandas as pd
import numpy as np

df = pd.read_csv('./data.csv')
df = df.sort_values(by=['deviceId', 'timestamp'])

def swap_columns(df, col1, col2):
    col_list = list(df.columns)
    x, y = col_list.index(col1), col_list.index(col2)
    col_list[y], col_list[x] = col_list[x], col_list[y]
    df = df[col_list]
    return df
df = swap_columns(df, 'pm2.5', 'lat')

def replace_negative_with_mean(df):
    # Iterate over each column
    for column in df.columns[3:]:
        if np.any(df[column] < 0):  # Check if column contains negative values
            column_mean = df[column].mean()  # Calculate column mean
            df[column] = np.where(df[column] < 0, column_mean, df[column])  # Replace negative values with mean
    
    return df
df = replace_negative_with_mean(df)

class PM25Dataset(Dataset):
    def __init__(self, data, sequence_length):
        self.data = data
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.data) - self.sequence_length

    def __getitem__(self, index):
        sequence = self.data[index:index+self.sequence_length, :]
        label = self.data[index+self.sequence_length, 0]  # Assuming that PM2.5 is the first column

        return sequence, label

sequence_length = 3
datasets = []
for device_id, group in df.groupby('deviceId'):
    group = group.drop(columns=['deviceId', 'timestamp', 'Unnamed: 0'])  # Drop non-feature columns
    group_values = group.values # 24 * 5
    dataset = PM25Dataset(group_values, sequence_length)
    datasets.append(dataset)

dataset = ConcatDataset(datasets)
train_size = int(0.8 * len(dataset))  # Let's use 80% of the data for training
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# init device to store data
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class PM25Predictor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(PM25Predictor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) 
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Hyperparameters
input_size = 5 # PM2.5, temperature, wind speed, longitude, latitude
hidden_size = 50
num_layers = 2
output_size = 1 # PM2.5
num_epochs = 100
learning_rate = 0.001


# Initialize the model, loss function, and optimizer
model = PM25Predictor(input_size, hidden_size, num_layers, output_size)
model = model.to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    for i, (sequences, labels) in enumerate(train_loader):
        sequences = sequences.to(device)
        labels = labels.to(device)
        
        sequences = sequences.to(torch.float32)
        labels = labels.to(torch.float32)

        outputs = model(sequences)
        labels = labels.view(outputs.shape)

        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')

model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient tracking
    total_predictions, correct_predictions = 0, 0
    for sequences, labels in test_loader:
        sequences = sequences.to(device)
        labels = labels.to(device)
        
        sequences = sequences.to(torch.float32)
        labels = labels.to(torch.float32)

        outputs = model(sequences)
        labels = labels.view(outputs.shape)

        loss = criterion(outputs, labels)
        predicted = outputs.round()  # Assuming PM2.5 can be rounded off to nearest integer
        total_predictions += labels.size(0)
        correct_predictions += (abs(predicted-labels)<=2).sum().item()

print('Test Accuracy of the model on the test images: {} %'.format((correct_predictions / total_predictions) * 100))